{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 201700949 설재혁\n",
    "\n",
    "## 1. 333 신경망 Foward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "은닉 계층의 입력값:  [1.16 0.42 0.62]\n",
      "은닉 계층의 결과값(=출력계층의 입력값):  [0.76133271 0.60348325 0.65021855]\n",
      "출력 계층의 결과값:  [0.97594736 0.88858496 1.25461119]\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid_function(x):\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "input_data = [0.9, 0.1, 0.8] # 입력값\n",
    "\n",
    "W_input_hidden = [[0.9, 0.3, 0.4], [0.2, 0.8, 0.2], [0.1, 0.5, 0.6]] # 입력 계층과 은닉 계층간의 가중치\n",
    "W_hidden_output = [[0.3, 0.7, 0.5], [0.6, 0.5, 0.2], [0.8, 0.1, 0.9]] # 은닉 계층과 출력 계층간의 가중치\n",
    "\n",
    "X_hidden = np.dot(W_input_hidden, input_data) # 은닉 계층의 입력값 = 초기 입력값과 가중치 간의 행렬곱\n",
    "\n",
    "input_data = sigmoid_function(X_hidden) # 은닉 계층의 결과값 == 출력 계층의 입력값\n",
    "\n",
    "X_output = np.dot(W_hidden_output, input_data) # 출력 계층 입력값과 가중치 간의 행렬곱\n",
    "\n",
    "output_data = sigmoid_function(X_output) # 출력 계층의 결과값\n",
    "\n",
    "print('은닉 계층의 입력값: ', X_hidden)\n",
    "print('은닉 계층의 결과값(=출력계층의 입력값): ', input_data)\n",
    "print('출력 계층의 결과값: ', X_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 222 신경망 Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "은닉 계층의 첫 번째 노드의 오차:  0.42000000000000004\n",
      "은닉 계층 두 번째 노드의 오차:  0.8800000000000001\n",
      "입력 계층의 첫 번째 노드의 오차:  0.3620000000000001\n",
      "입력 계층 두 번째 노드의 오차:  0.9380000000000002\n"
     ]
    }
   ],
   "source": [
    "e_output_1 = 0.8; e_output_2 = 0.5 # 출력 계층의 두 노드의 오차\n",
    "W_hidden_output= [[2.0, 3.0], [1.0, 4.0]]\n",
    "W_input_hidden = [[3.0, 2.0], [1.0, 7.0]]\n",
    "\n",
    "e_hidden_1 = e_output_1 * W_hidden_output[0][0]/np.sum(W_hidden_output[0]) + e_output_2 * W_hidden_output[1][0]/np.sum(W_hidden_output[1])\n",
    "e_hidden_2 = e_output_2 * W_hidden_output[1][1]/np.sum(W_hidden_output[1]) + e_output_1 * W_hidden_output[0][1]/np.sum(W_hidden_output[0])\n",
    "\n",
    "e_input_1 = e_hidden_1 * W_input_hidden[0][0]/np.sum(W_input_hidden[0]) + e_hidden_2 * W_input_hidden[1][0]/np.sum(W_input_hidden[1])\n",
    "e_input_2 = e_hidden_2 * W_input_hidden[1][1]/np.sum(W_input_hidden[1]) + e_hidden_1 * W_input_hidden[0][1]/np.sum(W_input_hidden[0])\n",
    "\n",
    "print('은닉 계층의 첫 번째 노드의 오차: ',e_hidden_1)\n",
    "print('은닉 계층 두 번째 노드의 오차: ', e_hidden_2)\n",
    "print('입력 계층의 첫 번째 노드의 오차: ',e_input_1)\n",
    "print('입력 계층 두 번째 노드의 오차: ', e_input_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 222 신경망 Weight Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
