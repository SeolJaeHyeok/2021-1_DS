{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 201700949 설재혁\n",
    "\n",
    "## 1. 333 신경망 Foward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "은닉 계층의 입력값:  [1.16 0.42 0.62]\n",
      "은닉 계층의 결과값(=출력계층의 입력값):  [0.76133271 0.60348325 0.65021855]\n",
      "출력 계층의 결과값:  [0.72630335 0.70859807 0.77809706]\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid_function(x):\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "input_data = [0.9, 0.1, 0.8] # 입력값\n",
    "\n",
    "W_input_hidden = [[0.9, 0.3, 0.4], [0.2, 0.8, 0.2], [0.1, 0.5, 0.6]] # 입력 계층과 은닉 계층간의 가중치\n",
    "W_hidden_output = [[0.3, 0.7, 0.5], [0.6, 0.5, 0.2], [0.8, 0.1, 0.9]] # 은닉 계층과 출력 계층간의 가중치\n",
    "\n",
    "X_hidden = np.dot(W_input_hidden, input_data) # 은닉 계층의 입력값 = 초기 입력값과 가중치 간의 행렬곱\n",
    "\n",
    "input_data = sigmoid_function(X_hidden) # 은닉 계층의 결과값 == 출력 계층의 입력값\n",
    "\n",
    "X_output = np.dot(W_hidden_output, input_data) # 출력 계층 입력값과 가중치 간의 행렬곱\n",
    "\n",
    "output_data = sigmoid_function(X_output) # 출력 계층의 결과값\n",
    "\n",
    "print('은닉 계층의 입력값: ', X_hidden)\n",
    "print('은닉 계층의 결과값(=출력계층의 입력값): ', input_data)\n",
    "print('출력 계층의 결과값: ', output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 222 신경망 Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "은닉 계층의 첫 번째 노드의 오차:  0.42000000000000004\n",
      "은닉 계층 두 번째 노드의 오차:  0.8800000000000001\n",
      "입력 계층의 첫 번째 노드의 오차:  0.3620000000000001\n",
      "입력 계층 두 번째 노드의 오차:  0.9380000000000002\n"
     ]
    }
   ],
   "source": [
    "e_output_1 = 0.8; e_output_2 = 0.5 # 출력 계층의 두 노드의 오차\n",
    "W_hidden_output= [[2.0, 3.0], [1.0, 4.0]]\n",
    "W_input_hidden = [[3.0, 2.0], [1.0, 7.0]]\n",
    "\n",
    "e_hidden_1 = e_output_1 * W_hidden_output[0][0]/np.sum(W_hidden_output[0]) + e_output_2 * W_hidden_output[1][0]/np.sum(W_hidden_output[1])\n",
    "e_hidden_2 = e_output_2 * W_hidden_output[1][1]/np.sum(W_hidden_output[1]) + e_output_1 * W_hidden_output[0][1]/np.sum(W_hidden_output[0])\n",
    "e_input_1 = e_hidden_1 * W_input_hidden[0][0]/np.sum(W_input_hidden[0]) + e_hidden_2 * W_input_hidden[1][0]/np.sum(W_input_hidden[1])\n",
    "e_input_2 = e_hidden_2 * W_input_hidden[1][1]/np.sum(W_input_hidden[1]) + e_hidden_1 * W_input_hidden[0][1]/np.sum(W_input_hidden[0])\n",
    "\n",
    "print('은닉 계층의 첫 번째 노드의 오차: ',e_hidden_1)\n",
    "print('은닉 계층 두 번째 노드의 오차: ', e_hidden_2)\n",
    "print('입력 계층의 첫 번째 노드의 오차: ',e_input_1)\n",
    "print('입력 계층 두 번째 노드의 오차: ', e_input_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 222 신경망 Weight Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오차 기울기:  -0.02650226143703718\n",
      "변화량:  -0.002650226143703718\n",
      "업데이트 된 가중치(W_11):  2.002650226143704\n"
     ]
    }
   ],
   "source": [
    "e_output_1 = 0.8 # 출력계층 오차\n",
    "alpha = 0.1 # 학습률 \n",
    "output_j = np.array([0.4, 0.5]) # 은닉계층에서의 결과 값\n",
    "W_hidden_output = np.array([2.0, 3.0]) # 은닉 계층과 출력 계층 간의 가중치\n",
    "sum_of_weight = np.dot(W_hidden_output, output_j) # 입력 신호의 가중치 합\n",
    "\n",
    "result = -(e_output_1 * sigmoid_function(sum_of_weight) * (1 - sigmoid_function(sum_of_weight)) * output_j[0]) # 오차 기울기\n",
    "variance = alpha * result # 변화량\n",
    "updatedWeight = W_hidden_output[0] - variance # 업데이트된 가중치\n",
    "\n",
    "print(\"오차 기울기: \", result)\n",
    "print(\"변화량: \", variance)\n",
    "print(\"업데이트 된 가중치(W_11): \", updatedWeight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 333 신경망 역전파 오차값과 가중치 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "출력 계층 오차 \n",
      " [[-0.716]\n",
      " [-0.698]\n",
      " [ 0.212]]\n",
      "\n",
      "은닉 계층 오차 \n",
      " [[-0.37113162]\n",
      " [-0.59081709]\n",
      " [-0.24005128]]\n",
      "\n",
      "입력 계층 오차 \n",
      " [[-0.4536006 ]\n",
      " [-0.41376828]\n",
      " [-0.33463112]]\n",
      "\n",
      "입력 신호의 가중치 합 \n",
      " [0.77 0.61 0.91]\n",
      "\n",
      "오차 기울기 \n",
      " [[ 0.06195407  0.08166464  0.08790945]\n",
      " [ 0.06039657  0.07961162  0.08569943]\n",
      " [-0.01834394 -0.02418003 -0.02602905]]\n",
      "\n",
      "변화량 \n",
      " [[ 0.00619541  0.00816646  0.00879094]\n",
      " [ 0.00603966  0.00796116  0.00856994]\n",
      " [-0.00183439 -0.002418   -0.00260291]]\n",
      "\n",
      "업데이트 전 가중치\n",
      " [[0.3 0.7 0.5]\n",
      " [0.6 0.5 0.2]\n",
      " [0.8 0.1 0.9]]\n",
      "\n",
      "업데이트 된 가중치\n",
      " [[0.29380459 0.69183354 0.49120906]\n",
      " [0.59396034 0.49203884 0.19143006]\n",
      " [0.80183439 0.102418   0.90260291]]\n"
     ]
    }
   ],
   "source": [
    "import scipy.special\n",
    "\n",
    "targets = np.array([[0.01], [0.01], [0.99]]) # 목표 값\n",
    "actuals = np.array([[0.726], [0.708], [0.778]]) # 실제 값\n",
    "\n",
    "W_input_hidden = np.array([[0.9, 0.3, 0.4], [0.2, 0.8, 0.2], [0.1, 0.5, 0.6]]) # 입력 계층과 은닉 계층간의 가중치\n",
    "W_hidden_output = np.array([[0.3, 0.7, 0.5], [0.6, 0.5, 0.2], [0.8, 0.1, 0.9]]) # 은닉 계층과 출력 계층간의 가중치\n",
    "\n",
    "E_output = targets - actuals # 오차\n",
    "print(\"출력 계층 오차 \\n\", E_output)\n",
    "\n",
    "W_hidden_output_sum1 = W_hidden_output.sum(axis=1, dtype=\"float\") # 은닉 계층과 출력 계층 간의 가중치 합\n",
    "W_hidden_output_norm = W_hidden_output.T / W_hidden_output_sum1 # 정규화\n",
    " \n",
    "E_hidden = np.dot(W_hidden_output_norm, E_output) # 은닉 계층 오차\n",
    "print('\\n은닉 계층 오차 \\n', E_hidden)\n",
    "\n",
    "W_input_hidden_sum1 = W_input_hidden.sum(axis=1,  dtype=\"float\") # 입력 계층과 은닉 계층 간의 가중치 합\n",
    "W_input_hidden_norm = W_hidden_output.T / W_input_hidden_sum1 # 정규화\n",
    "\n",
    "E_input = np.dot(W_hidden_output_norm, E_hidden) # 입력 계층 오차\n",
    "print('\\n입력 계층 오차 \\n', E_input)\n",
    "\n",
    "output_j = np.array([0.4, 0.5, 0.6]) # 은닉 계층에서의 결과값 (가공의 값)\n",
    "sum_of_weight = np.dot(W_hidden_output, output_j) # 입력 신호의 가중치 합 (아직 이해가 잘 안된다..)\n",
    "print('\\n입력 신호의 가중치 합 \\n', sum_of_weight)\n",
    "\n",
    "result = -(E_output * sigmoid_function(sum_of_weight) * (1 - sigmoid_function(sum_of_weight)) * output_j) # 오차 기울기\n",
    "print('\\n오차 기울기 \\n', result)\n",
    "variance = alpha * result # 변화량 == 학습률 * 오차 기울기\n",
    "print('\\n변화량 \\n', variance)\n",
    "\n",
    "print('\\n업데이트 전 가중치\\n', W_hidden_output)\n",
    "updatedWeight = W_hidden_output - variance # 업데이트 된 가중치\n",
    "print('\\n업데이트 된 가중치\\n', updatedWeight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
